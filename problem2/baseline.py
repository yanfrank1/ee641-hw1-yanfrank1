# -*- coding: utf-8 -*-
"""baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PAGRcsAGd30J_ZRqRbr7NahAa3g5xcl3
"""

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from dataset import KeypointDataset
from model import HeatmapNet
from evaluate import extract_keypoints_from_heatmaps, visualize_predictions

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train(model, loader, criterion, optimizer):
    model.train()
    total_loss = 0.0
    for images, targets in loader:
        images = images.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)

        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * images.size(0)

    return total_loss / len(loader.dataset)


def evaluate(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for images, targets in loader:
            images = images.to(DEVICE)
            targets = targets.to(DEVICE)

            outputs = model(images)
            loss = criterion(outputs, targets)

            total_loss += loss.item() * images.size(0)

    return total_loss / len(loader.dataset)

def ablation_study(dataset):
    """
    Conduct ablation studies on key hyperparameters.

    Experiments:
    1. Effect of heatmap resolution (32x32 vs 64x64 vs 128x128)
    2. Effect of Gaussian sigma (1.0, 2.0, 3.0, 4.0)
    3. Effect of skip connections (with vs without)
    """
    # Run experiments and save results
    os.makedirs("results/ablation", exist_ok=True)
    results = {"resolution": {}, "sigma": {}, "skip_connections": {}}

    # Resolution
    for res in [32, 64, 128]:
        ds = KeypointDataset(dataset.image_dir, dataset.annotation_file,
                             output_type="heatmap", heatmap_size=res, sigma=2.0)
        train_size = int(0.8 * len(ds))
        val_size = len(ds) - train_size
        train_ds, val_ds = random_split(ds, [train_size, val_size])
        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=32)
        model = HeatmapNet(num_keypoints=5, heatmap_size=res).to(DEVICE)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        for epoch in range(10):
            train(model, train_loader, criterion, optimizer)
        val_loss = evaluate(model, val_loader, criterion)
        results["resolution"][res] = val_loss
        print(f"[Ablation] Resolution {res} => Val Loss {val_loss:.4f}")

    # Sigma
    for sigma in [1.0, 2.0, 3.0, 4.0]:
        ds = KeypointDataset(dataset.image_dir, dataset.annotation_file,
                             output_type="heatmap", heatmap_size=64, sigma=sigma)
        train_size = int(0.8 * len(ds))
        val_size = len(ds) - train_size
        train_ds, val_ds = random_split(ds, [train_size, val_size])
        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=32)
        model = HeatmapNet(num_keypoints=5, heatmap_size=64).to(DEVICE)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        for epoch in range(10):
            train(model, train_loader, criterion, optimizer)
        val_loss = evaluate(model, val_loader, criterion)
        results["sigma"][sigma] = val_loss
        print(f"[Ablation] Sigma {sigma} => Val Loss {val_loss:.4f}")

    # Skip
    for skip in [True, False]:
        ds = KeypointDataset(dataset.image_dir, dataset.annotation_file,
                             output_type="heatmap", heatmap_size=64, sigma=2.0)
        train_size = int(0.8 * len(ds))
        val_size = len(ds) - train_size
        train_ds, val_ds = random_split(ds, [train_size, val_size])
        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=32)

        model = HeatmapNet(num_keypoints=5, skip=skip, heatmap_size=64).to(DEVICE)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        for epoch in range(10):
            train(model, train_loader, criterion, optimizer)
        val_loss = evaluate(model, val_loader, criterion)
        key = "with_skip" if skip else "no_skip"
        results["skip_connections"][key] = val_loss
        print(f"[Ablation] Skip {skip} => Val Loss {val_loss:.4f}")

    # Save results
    with open("results/ablation/ablation_results.json", "w") as f:
        json.dump(results, f, indent=4)
    pass


def analyze_failure_cases(heatmap_model, regression_model, val_loader_hm, val_loader_reg):
    """
    Identify and visualize failure cases.

    - Heatmap succeeds but regression fails
    - Regression succeeds but heatmap fails
    - Both fail
    """
    os.makedirs("results/visualizations/failures", exist_ok=True)
    heatmap_model.eval()
    regression_model.eval()
    criterion = nn.MSELoss()
    with torch.no_grad():
        for i, ((imgs_hm, gts_hm), (imgs_reg, gts_reg)) in enumerate(zip(val_loader_hm, val_loader_reg)):
            imgs_hm, gts_hm = imgs_hm.to(DEVICE), gts_hm.to(DEVICE)
            imgs_reg, gts_reg = imgs_reg.to(DEVICE), gts_reg.to(DEVICE)
            hm_outs = heatmap_model(imgs_hm)
            reg_outs = regression_model(imgs_reg)
            hm_coords = extract_keypoints_from_heatmaps(hm_outs).cpu().float()
            hm_gts = extract_keypoints_from_heatmaps(gts_hm).cpu().float()
            reg_coords = (reg_outs.view(-1, 5, 2) * 128.0).cpu().float()
            reg_gts = (gts_reg.view(-1, 5, 2) * 128.0).cpu().float()
            d_hm = torch.norm(hm_coords - hm_gts, dim=2).mean(1)
            d_reg = torch.norm(reg_coords - reg_gts, dim=2).mean(1)
            for j in range(min(4, imgs_hm.size(0))):
                case = None
                if d_hm[j] < 5 and d_reg[j] > 10:
                    case = "heatmap_good"
                elif d_reg[j] < 5 and d_hm[j] > 10:
                    case = "regression_good"
                elif d_reg[j] > 10 and d_hm[j] > 10:
                    case = "both_bad"

                if case:
                    img = imgs_hm[j].cpu().squeeze().numpy()
                    visualize_predictions(
                        img, reg_coords[j].numpy(), reg_gts[j].numpy(),
                        f"results/visualizations/failures/{case}_{i}_{j}.png"
                    )
    pass