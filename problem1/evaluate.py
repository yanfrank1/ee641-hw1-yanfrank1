# -*- coding: utf-8 -*-
"""evaluate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13bP29LFhT07xTOi8SftDNjiS_munchjn
"""

import torch
import os
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw
from utils import compute_iou, match_anchors_to_targets


def compute_ap(predictions, ground_truths, iou_threshold=0.5):
    """Compute Average Precision for a single class."""
    pred_boxes = predictions['boxes']
    pred_scores = predictions['scores']
    pred_labels = predictions['labels']
    gt_boxes = ground_truths['boxes']
    gt_labels = ground_truths['labels']
    if len(pred_boxes) == 0 or len(gt_boxes) == 0:
        return 0.0

    scores, order = torch.sort(pred_scores, descending=True)
    pred_boxes = pred_boxes[order]
    pred_labels = pred_labels[order]
    tp = torch.zeros(len(pred_boxes))
    fp = torch.zeros(len(pred_boxes))
    matched = torch.zeros(len(gt_boxes)).bool()
    for i, (pb, pl) in enumerate(zip(pred_boxes, pred_labels)):
        ious = compute_iou(pb.unsqueeze(0), gt_boxes).squeeze(0)
        max_iou, max_idx = ious.max(0)
        if max_iou >= iou_threshold and pl == gt_labels[max_idx] and not matched[max_idx]:
            tp[i] = 1
            matched[max_idx] = True
        else:
            fp[i] = 1

    tp_cum = torch.cumsum(tp, dim=0)
    fp_cum = torch.cumsum(fp, dim=0)
    precision = tp_cum / (tp_cum + fp_cum + 1e-6)
    recall = tp_cum / (len(gt_boxes) + 1e-6)
    ap = 0.0
    for t in torch.arange(0.0, 1.1, 0.1):
        if (recall >= t).any():
            ap += precision[recall >= t].max()
    return (ap / 11.0).item()

def visualize_detections(image, predictions, ground_truths, save_path):
    """Visualize predictions and ground truth boxes."""
    image = image.convert("RGB")
    draw = ImageDraw.Draw(image)

    for box in ground_truths["boxes"]:
        x1, y1, x2, y2 = box.tolist()
        draw.rectangle([x1, y1, x2, y2], outline="green", width=2)
    for i, box in enumerate(predictions["boxes"]):
        score = predictions["scores"][i].item()
        if score < 0.5:
            continue
        x1, y1, x2, y2 = box.tolist()
        draw.rectangle([x1, y1, x2, y2], outline="red", width=2)

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    image.save(save_path)
    pass

def analyze_scale_performance(model, dataloader, anchors, device):
    """Analyze which scales detect which object sizes."""
    # Generate statistics on detection performance per scale
    # Create visualizations showing scale specialization
    model.eval()
    scale_counts = [[], [], []]  # Detected box sizes per scale
    os.makedirs("results/visualizations/scale_analysis", exist_ok=True)

    with torch.no_grad():
        for i, (images, targets) in enumerate(dataloader):
            images = images.to(device)
            outputs = model(images)

            for b in range(len(images)):
                gt_boxes = targets[b]["boxes"].to(device)
                for scale_idx, preds in enumerate(outputs):
                    pred = preds[b]
                    C, H, W = pred.shape
                    pred = pred.permute(1, 2, 0).reshape(-1, pred.shape[1])
                    scale_anchors = anchors[scale_idx].to(pred.device)
                    scale_anchors = scale_anchors[:pred.shape[0]]
                    cxcy = pred[:, :2]
                    wh = pred[:, 2:4]
                    objectness = torch.sigmoid(pred[:, 4])
                    class_scores = torch.sigmoid(pred[:, 5:])
                    score, label = class_scores.max(dim=1)
                    score = score * objectness
                    keep = score > 0.8
                    if keep.sum() == 0:
                        continue
                    scale_anchors = scale_anchors[keep]
                    cxcy = cxcy[keep]
                    wh = wh[keep]
                    pred_cxcy = cxcy * scale_anchors[:, 2:] + scale_anchors[:, :2]
                    pred_wh = wh.exp() * scale_anchors[:, 2:]
                    box_xy1 = pred_cxcy - 0.5 * pred_wh
                    box_xy2 = pred_cxcy + 0.5 * pred_wh
                    pred_boxes = torch.cat([box_xy1, box_xy2], dim=1)
                    ious = compute_iou(pred_boxes, gt_boxes)
                    max_iou, _ = ious.max(dim=1)
                    matched = max_iou >= 0.5
                    sizes = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])
                    sizes = sizes[matched].cpu().numpy()
                    scale_counts[scale_idx].extend(sizes)

    for idx, sizes in enumerate(scale_counts):
        plt.figure()
        plt.hist(sizes, bins=30)
        plt.title(f"Scale {idx + 1} - Detected Object Sizes")
        plt.xlabel("Box Area (pixels^2)")
        plt.ylabel("Count")
        plt.savefig(f"results/visualizations/scale_analysis/scale_{idx+1}_size_distribution.png")
        plt.close()
    pass